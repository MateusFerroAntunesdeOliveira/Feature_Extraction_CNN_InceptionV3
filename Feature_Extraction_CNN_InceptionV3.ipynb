{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Extraction_CNN_InceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAONhdWCaFcXj3YXqbxZ9l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MateusFerroAntunesdeOliveira/Feature_Extraction_CNN_InceptionV3/blob/main/Feature_Extraction_CNN_InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4dAI2wmXGFt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfvEDPE7XXVg"
      },
      "source": [
        "# ------ IMPORTS ------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as pl\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from skimage import feature\n",
        "from skimage.feature import hog\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Monolithic Classifiers \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Ensembles\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmgy9z_DXZB6"
      },
      "source": [
        "\n",
        "model = InceptionV3(include_top = False, weights = 'imagenet', pooling = 'avg', input_tensor=Input(shape=(299,299,3)))\n",
        "\n",
        "# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= List of paths =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "file_list = []\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/humanos\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/praia\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/obras\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/onibus\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/dino\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/elefante\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/flores\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/cavalos\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/montanhas\"))\n",
        "file_list.append(os.listdir(r\"/content/drive/MyDrive/ML_TDE_02/Base/comida\"))\n",
        "\n",
        "# General path\n",
        "path = '/content/drive/MyDrive/ML_TDE_02/Base/'\n",
        "\n",
        "# List of classes\n",
        "class_names = ['humanos', 'praia', 'obras', 'onibus', 'dino', 'elefante', 'flores', 'cavalos', 'montanhas', 'comida'] \n",
        "\n",
        "# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Feature extraction =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "X = []\n",
        "X_deep = []\n",
        "y = []\n",
        "\n",
        "for classes_files, classe in zip (file_list, range(10)):\n",
        "    for i in range(100):\n",
        "      name= str(path) + str(class_names[classe]) + str('/') + str(classes_files[i]) \n",
        "      print(name)\n",
        "      imagem = cv2.imread(name)\n",
        "\n",
        "      print(imagem.shape)\n",
        "      altura, largura, _ = imagem.shape\n",
        "    \n",
        "      # -------------- Convert the image to RGB and Gray -------------- \n",
        "      cinza = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
        "      rgb   = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      # -------------- Color Histograms -------------- \n",
        "      r_histograma = cv2.calcHist([rgb], [0], None, [256], [0, 256])/(altura*largura)\n",
        "      g_histograma = cv2.calcHist([rgb], [1], None, [256], [0, 256])/(altura*largura)\n",
        "      b_histograma = cv2.calcHist([rgb], [2], None, [256], [0, 256])/(altura*largura)\n",
        "\n",
        "      # -------------- Local Binary Pattern (LBP) -------------- \n",
        "      lbp = feature.local_binary_pattern(cinza, 59, 1, method = \"uniform\")\n",
        "      (lbp_histograma, _) = np.histogram(lbp.ravel(), bins=59, range=(0, 59))\n",
        "      lbp_histograma = lbp_histograma.astype(\"float\")\n",
        "      lbp_histograma /= (lbp_histograma.sum())\n",
        "    \n",
        "      # -------------- Hog (Histogram of Gradient - Direction) -------------- \n",
        "      hg = hog(cinza, orientations=8, pixels_per_cell=(32, 32), cells_per_block=(8, 8), block_norm='L2-Hys')\n",
        "    \n",
        "      # -------------- Concatenate the handcrafted feature sets -------------- \n",
        "      X_image = [lbp_histograma, hg, r_histograma, g_histograma, b_histograma]    \n",
        "      X_image_aux = []\n",
        "      for aux in X_image:\n",
        "          X_image_aux = np.append(X_image_aux, np.ravel(aux))\n",
        "    \n",
        "      X_image = [i for i in X_image_aux]\n",
        "      y.append(classe)\n",
        "      X.append(X_image)\n",
        "      \n",
        "      # -------------- Extract deep features using InceptionV3 pretrained model -------------- \n",
        "      img = cv2.resize(imagem,(299,299))\n",
        "      xd = image.img_to_array(img)\n",
        "      xd = np.expand_dims(xd, axis=0)\n",
        "      xd = preprocess_input(xd)\n",
        "      deep_features = model.predict(xd)\n",
        "      print(deep_features.shape)\n",
        "      \n",
        "      X_image_aux = []\n",
        "      for aux in deep_features:\n",
        "          X_image_aux = np.append(X_image_aux, np.ravel(aux))\n",
        "    \n",
        "      deep_features = [i for i in X_image_aux]\n",
        "      X_deep.append(deep_features)\n",
        "            \n",
        "\n",
        "# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Saving the files/folders =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "# Saving the extracted features (handcrafted) in a csv file\n",
        "df = pd.DataFrame(X)\n",
        "df.to_csv('X.csv', header = False, index = False)\n",
        "\n",
        "# Saving the extracted features (deep) in a csv file\n",
        "df = pd.DataFrame(X_deep)\n",
        "df.to_csv('X_deep.csv', header = False, index = False)\n",
        "\n",
        "# Saving the classes in a csv file\n",
        "df_class = pd.DataFrame(y)\n",
        "df_class.to_csv('y.csv', header = False, index = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xFXIYr0XfUJ"
      },
      "source": [
        "# Labels\n",
        "y = pd.read_csv('/content/drive/MyDrive/ML_TDE_02/Features/y.csv', header=None)\n",
        "y = y.to_numpy()\n",
        "y = np.ravel(y)\n",
        "print(y.shape)\n",
        "\n",
        "# Deep features\n",
        "X = pd.read_csv('/content/drive/MyDrive/ML_TDE_02/Features/X_deep.csv', header=None)\n",
        "X = X.to_numpy()\n",
        "print(X.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sYyhzcKXiV2"
      },
      "source": [
        "# ------------ Plot graphic ------------\n",
        "\n",
        "matplotlib.rc('figure', figsize = (11, 11))\n",
        "\n",
        "# Plotting confusion matrix\n",
        "def plot_confusion_matrix(ax, cm, title = None):\n",
        "    df_cm = pd.DataFrame(cm, index = [i for i in \"0123456789\"], columns = [i for i in \"0123456789\"])\n",
        "    sns.heatmap(df_cm, annot = True)\n",
        "    ax.set_title('Confusion Matrix --> ' + title)\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "    return ax\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUZ0P_IrXkp3"
      },
      "source": [
        "# -------- HOLDOUT --------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# Monolithic Classifiers\n",
        "knn = KNeighborsClassifier(n_neighbors = 1, p = 3, algorithm = 'brute', weights = 'distance')\n",
        "dt =  DecisionTreeClassifier(criterion = 'entropy', max_depth = 10, min_samples_split = 10)\n",
        "svm = SVC(probability = True, C = 0.1, kernel = 'linear')\n",
        "nb = GaussianNB(var_smoothing = 1e-09)\n",
        "mlp = MLPClassifier(hidden_layer_sizes = (16), activation = 'relu', max_iter = 5000, solver = 'lbfgs', tol =  1e-10, early_stopping = True, validation_fraction = 0.2)\n",
        "\n",
        "# Ensembles (homogeneous)\n",
        "bg = BaggingClassifier(base_estimator = mlp, n_estimators = 100, random_state = 0)\n",
        "ada = AdaBoostClassifier(n_estimators = 100, base_estimator = nb, learning_rate = 0.05, random_state = 0)\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 0)\n",
        "\n",
        "# Ensembles (heterogeneous)\n",
        "cb = VotingClassifier(estimators = [('DecisionTree', dt), ('NaiveBayes', nb)], voting = 'soft')\n",
        "\n",
        "titles = ['KNN','DecisionTree', 'SVM', 'NaiveBayes', 'MLP', 'Bag', 'Ada', 'RF', 'DT+NB']\n",
        "methods = [knn, dt, svm, nb, mlp, bg, ada, rf, cb]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNW5OGKTXprq"
      },
      "source": [
        "# Fit the classifiers\n",
        "scores = []\n",
        "for method, name in zip(methods, titles):\n",
        "    method.fit(X_train, y_train)\n",
        "    scores.append(method.score(X_test, y_test))\n",
        "    print(\"Classification Accuracy {} = {}\".format(name, method.score(X_test, y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q28HfEOYjBR"
      },
      "source": [
        "# Plotting Confusion matrix\n",
        "\n",
        "fig3, sub1 = plt.subplots(5, 2, figsize = (15, 15))\n",
        "plt.subplots_adjust(wspace = 0.4, hspace = 0.4)\n",
        "for clf, ax, title in zip(methods, sub1.flatten(), titles):\n",
        "    y_predicted = clf.predict(X_test)\n",
        "    cm = confusion_matrix(y_test , y_predicted)\n",
        "    df_cm = pd.DataFrame(cm, index = [i for i in \"0123456789\"], columns = [i for i in \"0123456789\"])\n",
        "    sns.heatmap(df_cm, annot=True, ax=ax)\n",
        "    ax.set_title('Confusion Matrix --> ' + title)\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "\n",
        "plt.show()\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}